{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "from src.lib import version_extractor, name_extractor\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pypi_data = pd.read_json('../../data/repology/pypicache.json')\n",
    "pypi_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Converting Info JSON to a DataFrame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pypi_data['info'].map(lambda x: x['author'])\n",
    "# df = pypi_data['info'].apply(pd.Series) --- Slow in general. For the given case, just as fast as json_normalize (without a set max level) as that attempts to unnest the entire json object. ~ 30 seconds\n",
    "# df = pd.json_normalize(pypi_data['info']) --- Slow due to previously mentioned unnesting. ~ 30 seconds\n",
    "# df = pd.json_normalize(pypi_data['info'], max_level=0) --- Faster. ~ 5 seconds\n",
    "# df = pd.DataFrame(pypi_data['info'].values.tolist()) --- Fastest. ~ 0.5 seconds\n",
    "\n",
    "info_df = pd.DataFrame(pypi_data['info'].values.tolist())\n",
    "info_df = info_df[['name', 'version', 'requires_dist', 'author']]\n",
    "# Rename headers to make it more readable\n",
    "info_df.rename(columns={'requires_dist': 'dependency', }, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_df: DataFrame = info_df.sort_values(by=['name', 'version'], ascending=[True, False], ignore_index=True)\n",
    "sorted_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_date_from_nested_releases_json(releases_json):\n",
    "    if isinstance(releases_json, dict):\n",
    "        latest_release = [*releases_json.values()][0]\n",
    "        if latest_release:\n",
    "            return latest_release[0]['upload_time']\n",
    "    else:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "upload_time_series: Series = pypi_data['releases'].map(extract_date_from_nested_releases_json)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_df.insert(loc=2, column='upload_time', value=upload_time_series)\n",
    "sorted_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_df = sorted_df.explode('dependency').reset_index(drop=True)\n",
    "sorted_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extracting information from the dependency string"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dependency_version_series = sorted_df['dependency'].apply(version_extractor)\n",
    "dependency_name_series = sorted_df['dependency'].apply(name_extractor)\n",
    "\n",
    "# Used just for visual purposes\n",
    "dependencies_df = pd.concat([dependency_name_series, dependency_version_series], axis=1, ignore_index=True)\n",
    "dependencies_df.columns = ['dependency_name', 'dependency_version']\n",
    "dependencies_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_df['dependency'] = dependency_name_series\n",
    "sorted_df.insert(4, 'dependency_version', dependency_version_series)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def convert_to_normalized_format(grouped_df: DataFrame):\n",
    "    # print(grouped_df)\n",
    "    normalized_form = {\n",
    "        # We know the name is the same for all rows\n",
    "        'name': grouped_df['name'].iloc[0],\n",
    "        'versions': {}\n",
    "    }\n",
    "    for index, version in enumerate(grouped_df['version']):\n",
    "        normalized_form['versions'][version] = {\n",
    "            'timestamp': grouped_df['upload_time'].iloc[index],\n",
    "            'dependencies': {}\n",
    "        }\n",
    "        for dependency, dependency_version in zip(grouped_df['dependency'], grouped_df['dependency_version']):\n",
    "            normalized_form['versions'][version]['dependencies'][dependency] = dependency_version\n",
    "\n",
    "    return normalized_form\n",
    "\n",
    "\n",
    "normalized_df: DataFrame = sorted_df.copy().dropna()\n",
    "normalized_json_df = normalized_df.groupby('name').apply(convert_to_normalized_format)\n",
    "normalized_json_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the processed data to file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normalized_df.to_csv('../../data/output/pypi-repology-dependencies.csv', index=False)\n",
    "normalized_json_df.to_json('../../data/output/pypi-repology-dependencies.json', orient='records')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Converting Releases JSON to a DataFrame\n",
    "Decided against using releases as they do not contain meaningful information. Most of the time they only contain the most recent version that can be recovered from the info JSON"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# releases_df = pd.DataFrame(pypi_data['releases'].values.tolist()) --- Runs out of memory\n",
    "# releases_df = pypi_data['releases'].map(lambda x: x.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bits and bobs that were tinkered with but were scrapped"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test = pypi_data['info'].apply(lambda el: json.loads(json.dumps(el)))\n",
    "# test\n",
    "# pypi_data['info'][0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pypi_data['info'].to_json('../../data/repology/pypi_info.json', orient='records', lines=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pypi_data_reduced = pd.read_json('../../data/repology/pypi_info.json', orient='records', lines=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pypi_data_reduced.dropna(subset=['requires_dist'], inplace=True)\n",
    "# pypi_data_reduced.reset_index(drop=True, inplace=True)\n",
    "# # pypi_data_reduced['requires_dist'] = pypi_data_reduced['requires_dist'].apply(json.loads)\n",
    "# pypi_data_reduced['requires_dist'] = [','.join(x) for x in pypi_data_reduced['requires_dist']]\n",
    "# pypi_data_reduced[['name', 'requires_dist']]\n",
    "# Select from pypi_data_reduced all the data that has name zzzzls-Spider\n",
    "# pypi_data_reduced[pypi_data_reduced['name'] == 'pandas']['requires_dist'].values[0]\n",
    "# pypi_data_reduced['']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with open('../../data/repology/pypicache.json', 'r') as file:\n",
    "#     json_data = json.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# nested_json_data = pd.json_normalize(json_data, max_level=2)\n",
    "# nested_json_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# chunks = pd.read_json('../../data/repology/pypicache.json', lines=True, chunksize=100000)\n",
    "#\n",
    "# for chunk in chunks:\n",
    "#     display(chunk)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initial method for converting to a normalized format. Was hard to read, and it contained quite a few bugs. Decided it was better to rewrite it\n",
    "# def convert_to_normalized_format(grouped_df: DataFrameGroupBy):\n",
    "#     return_list = []\n",
    "#     for _, rows in grouped_df:\n",
    "#         inner_dict = {\n",
    "#             'name': rows['name'].values[0],\n",
    "#             'versions': {\n",
    "#                 rows['version'].values[0]: {\n",
    "#                     'timestamp': rows['upload_time'].values[0],\n",
    "#                     'dependencies': {}\n",
    "#                 }\n",
    "#             }\n",
    "#         }\n",
    "#         for dep, v in zip(rows['dependency'].values, rows['dependency_version'].values):\n",
    "#             inner_dict['versions'][rows['version'].values[0]]['dependencies'] |= {dep: v}\n",
    "#         return_list.append(inner_dict)\n",
    "#     return pd.DataFrame(return_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# normalized_df: DataFrame = info_df.copy().dropna()\n",
    "# display(normalized_df.loc[normalized_df['name'] == '024travis-test024'])\n",
    "# normalized_json_df = normalized_df.groupby('name').pipe(convert_to_normalized_format)\n",
    "# normalized_json_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Attempt to use multiprocessing. Ended up not using it since I discovered that using built-in strings is extremely fast compared to regex.\n",
    "# cores = multiprocessing.cpu_count()\n",
    "# chunks = np.array_split(info_df['dependency'], cores)\n",
    "#\n",
    "# with Pool(cores) as pool:\n",
    "#     processed = pd.concat(pool.map(extract_semantic_version, chunks), ignore_index=True)\n",
    "\n",
    "# processed\n",
    "# info_df.dependency.str.extract(compiled_rx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}